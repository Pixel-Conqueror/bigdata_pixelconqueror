{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1856c28f-12c9-4ef6-9cb1-5b46c1c60e7a",
   "metadata": {},
   "source": [
    "# Pipeline Batch ETL MovieLens\r\n",
    "\r\n",
    "Ce notebook exécute, étape par étape, l’ETL batch pour préparer les données MovieLens à l’entraînement d’un modèle ALS robuste.\r\n",
    "\r\n",
    "**Étapes**  \r\n",
    "1. Configuration & lecture des données  \r\n",
    "2. Analyse exploratoire rapide  \r\n",
    "3. Nettoyage avancé  \r\n",
    "4. Enrichissement temporel  \r\n",
    "5. Écriture en Parquet  \r\n",
    "6. Validation\r\n",
    "arquet\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "847fff8a-8ab5-4d38-8749-bb2772e67cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 1 – Imports & SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, dayofmonth, count, mean, stddev\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BatchETLPipelineEnhanced\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d842237f-870e-4ba4-ad38-cf9ffd922fe1",
   "metadata": {},
   "source": [
    "## 1. Lecture des données brutes\r\n",
    "\r\n",
    "On charge les CSV depuis HDFS et on affiche quelques lignes.\r\n",
    ".\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4272955e-38aa-4822-a592-2bb1b8827c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Raw films  : 27278\n",
      "🔍 Raw notes  : 20000263\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "|movieId|title                             |genres                                     |\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "|1      |Toy Story (1995)                  |Adventure|Animation|Children|Comedy|Fantasy|\n",
      "|2      |Jumanji (1995)                    |Adventure|Children|Fantasy                 |\n",
      "|3      |Grumpier Old Men (1995)           |Comedy|Romance                             |\n",
      "|4      |Waiting to Exhale (1995)          |Comedy|Drama|Romance                       |\n",
      "|5      |Father of the Bride Part II (1995)|Comedy                                     |\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-------+------+-------------------+\n",
      "|userId|movieId|rating|timestamp          |\n",
      "+------+-------+------+-------------------+\n",
      "|1     |2      |3.5   |2005-04-02 23:53:47|\n",
      "|1     |29     |3.5   |2005-04-02 23:31:16|\n",
      "|1     |32     |3.5   |2005-04-02 23:33:39|\n",
      "|1     |47     |3.5   |2005-04-02 23:32:07|\n",
      "|1     |50     |3.5   |2005-04-02 23:29:40|\n",
      "+------+-------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_raw = spark.read.csv(\n",
    "    \"hdfs://namenode:9000/movielens/raw/movies/movies.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "ratings_raw = spark.read.csv(\n",
    "    \"hdfs://namenode:9000/movielens/raw/ratings/ratings.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "print(f\"🔍 Raw films  : {movies_raw.count()}\")\n",
    "print(f\"🔍 Raw notes  : {ratings_raw.count()}\")\n",
    "\n",
    "movies_raw.show(5, truncate=False)\n",
    "ratings_raw.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd9cd23-3a61-4499-81d8-b2ee5cd5a723",
   "metadata": {},
   "source": [
    "## 2. Analyse exploratoire rapide\n",
    "\n",
    "On regarde la distribution des notes pour détecter d’éventuels outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8de6ede-d5f8-47e9-9583-8e4707aef714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean=3.526, StdDev=1.052\n",
      "+------+-------+\n",
      "|rating|  count|\n",
      "+------+-------+\n",
      "|   0.5| 239125|\n",
      "|   1.0| 680732|\n",
      "|   1.5| 279252|\n",
      "|   2.0|1430997|\n",
      "|   2.5| 883398|\n",
      "|   3.0|4291193|\n",
      "|   3.5|2200156|\n",
      "|   4.0|5561926|\n",
      "|   4.5|1534824|\n",
      "|   5.0|2898660|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stats = ratings_raw.select(\n",
    "    mean(\"rating\").alias(\"mean\"),\n",
    "    stddev(\"rating\").alias(\"stddev\")\n",
    ").first()\n",
    "mean_rating, stddev_rating = stats[\"mean\"], stats[\"stddev\"]\n",
    "print(f\"Mean={mean_rating:.3f}, StdDev={stddev_rating:.3f}\")\n",
    "\n",
    "ratings_raw.groupBy(\"rating\").count().orderBy(\"rating\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347fcc79-ba65-4258-8121-6b9d8a344842",
   "metadata": {},
   "source": [
    "## 3. Stratégie de nettoyage avancé\r\n",
    "\r\n",
    "Pour maximiser la qualité de l’ALS, on :\r\n",
    "1. Calcule le **z-score** des notes par utilisateur et exclut tout `|z| > 3`.  \r\n",
    "2. Filtre ensuite les notes hors de l’intervalle global `[μ ± 3σ]`.  \r\n",
    "3. Élimine les utilisateurs ayant < 20 interactions nettes et les films ayant < 50 interactions nettes.  \r\n",
    "4. Supprime doublons et nulls.\r\n",
    "\r\n",
    "Cette double détection d’outliers (global + par utilisateur) rendra le modèle plus robuste aux comportements extrêmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fbd5e50-6ec0-448a-89a8-22228eabb377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Notes après nettoyage : 19730315\n",
      "+-------+------+------+-------------------+------------------+------------------+-------------------+-----+-----+\n",
      "|movieId|userId|rating|timestamp          |μ_u               |σ_u               |z_score            |count|count|\n",
      "+-------+------+------+-------------------+------------------+------------------+-------------------+-----+-----+\n",
      "|1      |31    |3.0   |2015-02-23 23:18:07|3.3760162601626016|1.5661841004394155|-0.2400843298416226|246  |49638|\n",
      "|110    |31    |5.0   |2015-02-23 23:17:53|3.3760162601626016|1.5661841004394155|1.036904754288954  |246  |53622|\n",
      "|260    |31    |5.0   |2015-02-23 23:17:13|3.3760162601626016|1.5661841004394155|1.036904754288954  |246  |54364|\n",
      "+-------+------+------+-------------------+------------------+------------------+-------------------+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, stddev, expr\n",
    "\n",
    "# 1. Stats globales\n",
    "stats = ratings_raw.select(mean(\"rating\").alias(\"μ\"), stddev(\"rating\").alias(\"σ\")).first()\n",
    "μ, σ = stats[\"μ\"], stats[\"σ\"]\n",
    "\n",
    "# 2. Z-score par utilisateur\n",
    "user_stats = ratings_raw.groupBy(\"userId\") \\\n",
    "    .agg(avg(\"rating\").alias(\"μ_u\"), stddev(\"rating\").alias(\"σ_u\"))\n",
    "\n",
    "ratings_z = ratings_raw.join(user_stats, \"userId\") \\\n",
    "    .withColumn(\"z_score\", (col(\"rating\") - col(\"μ_u\"))/col(\"σ_u\"))\n",
    "\n",
    "# 3. Filtrage z-score et global\n",
    "clean1 = ratings_z.filter((col(\"z_score\").between(-3,3))) \\\n",
    "    .filter((col(\"rating\") >= μ - 3*σ) & (col(\"rating\") <= μ + 3*σ))\n",
    "\n",
    "# 4. Compter interactions nettes\n",
    "user_counts = clean1.groupBy(\"userId\").count().alias(\"user_count\")\n",
    "movie_counts = clean1.groupBy(\"movieId\").count().alias(\"movie_count\")\n",
    "\n",
    "# 5. Exclure les petits volumes\n",
    "clean2 = clean1.join(user_counts.filter(col(\"count\")>=20), \"userId\") \\\n",
    "               .join(movie_counts.filter(col(\"count\")>=50), \"movieId\")\n",
    "\n",
    "# 6. Suppression nulls/doublons\n",
    "ratings_clean = clean2 \\\n",
    "    .dropna(how=\"any\", subset=[\"userId\",\"movieId\",\"rating\",\"timestamp\"]) \\\n",
    "    .dropDuplicates([\"userId\",\"movieId\",\"timestamp\"]) \\\n",
    "    .cache()\n",
    "\n",
    "count_after = ratings_clean.count()\n",
    "print(f\"📊 Notes après nettoyage : {count_after}\")\n",
    "ratings_clean.show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9e980a-9ff2-4629-ae2a-cda8658f820e",
   "metadata": {},
   "source": [
    "## 4. Enrichissement temporel\n",
    "\n",
    "Extraire année, mois, jour et convertir `timestamp` en date si nécessaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bda0fa2-67ae-405f-8154-9b082831fbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+-------------------+------------------+------------------+-------------------+-----+-----+----+-----+---+\n",
      "|movieId|userId|rating|timestamp          |μ_u               |σ_u               |z_score            |count|count|year|month|day|\n",
      "+-------+------+------+-------------------+------------------+------------------+-------------------+-----+-----+----+-----+---+\n",
      "|1      |31    |3.0   |2015-02-23 23:18:07|3.3760162601626016|1.5661841004394155|-0.2400843298416226|246  |49638|2015|2    |23 |\n",
      "|110    |31    |5.0   |2015-02-23 23:17:53|3.3760162601626016|1.5661841004394155|1.036904754288954  |246  |53622|2015|2    |23 |\n",
      "|260    |31    |5.0   |2015-02-23 23:17:13|3.3760162601626016|1.5661841004394155|1.036904754288954  |246  |54364|2015|2    |23 |\n",
      "|364    |31    |3.0   |2015-02-25 06:13:27|3.3760162601626016|1.5661841004394155|-0.2400843298416226|246  |38946|2015|2    |25 |\n",
      "|527    |31    |0.5   |2015-02-23 23:19:58|3.3760162601626016|1.5661841004394155|-1.8363206850048432|246  |49972|2015|2    |23 |\n",
      "+-------+------+------+-------------------+------------------+------------------+-------------------+-----+-----+----+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_enriched = (ratings_clean\n",
    "    .withColumn(\"year\",  year(col(\"timestamp\")))\n",
    "    .withColumn(\"month\", month(col(\"timestamp\")))\n",
    "    .withColumn(\"day\",   dayofmonth(col(\"timestamp\")))\n",
    ")\n",
    "ratings_enriched.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af28a42f-bac7-45c8-87e4-3af78b55711a",
   "metadata": {},
   "source": [
    "## 5. Réduction du nombre de partitions Parquet\r\n",
    "\r\n",
    "Pour éviter de générer des milliers de petits fichiers, on repartitionne avant écriture :\r\n",
    "\r\n",
    "- **Movies** : 1 partition  \r\n",
    "- **Ratings** : `year,month,day`, mais on limite à ~ 10 partitions en coalesçant sur la date.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86649642-a0ca-4377-986f-a3403727e873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Champs finaux : ['userId', 'movieId', 'rating', 'timestamp', 'year', 'month', 'day']\n",
      "+------+-------+------+-------------------+----+-----+---+\n",
      "|userId|movieId|rating|timestamp          |year|month|day|\n",
      "+------+-------+------+-------------------+----+-----+---+\n",
      "|31    |1      |3.0   |2015-02-23 23:18:07|2015|2    |23 |\n",
      "|31    |110    |5.0   |2015-02-23 23:17:53|2015|2    |23 |\n",
      "|31    |260    |5.0   |2015-02-23 23:17:13|2015|2    |23 |\n",
      "+------+-------+------+-------------------+----+-----+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_final = ratings_enriched.select(\n",
    "    \"userId\",\"movieId\",\"rating\",\"timestamp\",\"year\",\"month\",\"day\"\n",
    ")\n",
    "\n",
    "# Vérif\n",
    "print(\"Champs finaux :\", ratings_final.columns)\n",
    "ratings_final.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c230f4da-ef73-45fe-8866-a2a27c290ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 Écriture CSV terminée\n"
     ]
    }
   ],
   "source": [
    "# Movies en 1 unique CSV\n",
    "movies_raw.repartition(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\", True) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"hdfs://namenode:9000/movielens/processed/batch/movies_csv\")\n",
    "\n",
    "# Ratings en 1 unique CSV\n",
    "ratings_final.coalesce(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\", True) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"hdfs://namenode:9000/movielens/processed/batch/ratings_csv\")\n",
    "\n",
    "print(\"🎉 Écriture CSV terminée\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc73feb-d316-4e37-8fa7-9342ecb8ec75",
   "metadata": {},
   "source": [
    "## 6. Validation des Parquet\n",
    "\n",
    "Vérification rapide des `count()` et aperçu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cbcd943-cc1a-414e-b262-6730c60ea143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Films CSV   : 27278\n",
      "✔️ Notes CSV   : 19730315\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "|movieId|title                             |genres                                     |\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "|1      |Toy Story (1995)                  |Adventure|Animation|Children|Comedy|Fantasy|\n",
      "|2      |Jumanji (1995)                    |Adventure|Children|Fantasy                 |\n",
      "|3      |Grumpier Old Men (1995)           |Comedy|Romance                             |\n",
      "|4      |Waiting to Exhale (1995)          |Comedy|Drama|Romance                       |\n",
      "|5      |Father of the Bride Part II (1995)|Comedy                                     |\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-------+------+-------------------+----+-----+---+\n",
      "|userId|movieId|rating|timestamp          |year|month|day|\n",
      "+------+-------+------+-------------------+----+-----+---+\n",
      "|31    |1      |3.0   |2015-02-23 23:18:07|2015|2    |23 |\n",
      "|31    |110    |5.0   |2015-02-23 23:17:53|2015|2    |23 |\n",
      "|31    |260    |5.0   |2015-02-23 23:17:13|2015|2    |23 |\n",
      "|31    |364    |3.0   |2015-02-25 06:13:27|2015|2    |25 |\n",
      "|31    |527    |0.5   |2015-02-23 23:19:58|2015|2    |23 |\n",
      "+------+-------+------+-------------------+----+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lecture des movies depuis CSV\n",
    "df_movies = spark.read.csv(\n",
    "    \"hdfs://namenode:9000/movielens/processed/batch/movies_csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# Lecture des ratings depuis CSV\n",
    "df_ratings = spark.read.csv(\n",
    "    \"hdfs://namenode:9000/movielens/processed/batch/ratings_csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "print(f\"✔️ Films CSV   : {df_movies.count()}\")\n",
    "print(f\"✔️ Notes CSV   : {df_ratings.count()}\")\n",
    "\n",
    "df_movies.show(5, truncate=False)\n",
    "df_ratings.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73996784-b58c-423a-a953-bbd301ef91ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972c67a-6a4b-4b5f-8405-640b9633912a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
