{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1856c28f-12c9-4ef6-9cb1-5b46c1c60e7a",
   "metadata": {},
   "source": [
    "# Pipeline Batch ETL MovieLens\r\n",
    "\r\n",
    "Ce notebook ex√©cute, √©tape par √©tape, l‚ÄôETL batch pour pr√©parer les donn√©es MovieLens √† l‚Äôentra√Ænement d‚Äôun mod√®le ALS robuste.\r\n",
    "\r\n",
    "**√âtapes**  \r\n",
    "1. Configuration & lecture des donn√©es  \r\n",
    "2. Analyse exploratoire rapide  \r\n",
    "3. Nettoyage avanc√©  \r\n",
    "4. Enrichissement temporel  \r\n",
    "5. √âcriture en Parquet  \r\n",
    "6. Validation\r\n",
    "arquet\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "847fff8a-8ab5-4d38-8749-bb2772e67cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 1 ‚Äì Imports & SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, dayofmonth, count, mean, stddev\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BatchETLPipelineEnhanced\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d842237f-870e-4ba4-ad38-cf9ffd922fe1",
   "metadata": {},
   "source": [
    "## 1. Lecture des donn√©es brutes\r\n",
    "\r\n",
    "On charge les CSV depuis HDFS et on affiche quelques lignes.\r\n",
    ".\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4272955e-38aa-4822-a592-2bb1b8827c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Raw films  : 27278\n",
      "üîç Raw notes  : 20000263\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "|movieId|title                             |genres                                     |\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "|1      |Toy Story (1995)                  |Adventure|Animation|Children|Comedy|Fantasy|\n",
      "|2      |Jumanji (1995)                    |Adventure|Children|Fantasy                 |\n",
      "|3      |Grumpier Old Men (1995)           |Comedy|Romance                             |\n",
      "|4      |Waiting to Exhale (1995)          |Comedy|Drama|Romance                       |\n",
      "|5      |Father of the Bride Part II (1995)|Comedy                                     |\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-------+------+-------------------+\n",
      "|userId|movieId|rating|timestamp          |\n",
      "+------+-------+------+-------------------+\n",
      "|1     |2      |3.5   |2005-04-02 23:53:47|\n",
      "|1     |29     |3.5   |2005-04-02 23:31:16|\n",
      "|1     |32     |3.5   |2005-04-02 23:33:39|\n",
      "|1     |47     |3.5   |2005-04-02 23:32:07|\n",
      "|1     |50     |3.5   |2005-04-02 23:29:40|\n",
      "+------+-------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_raw = spark.read.csv(\n",
    "    \"hdfs://namenode:9000/movielens/raw/movies/movies.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "ratings_raw = spark.read.csv(\n",
    "    \"hdfs://namenode:9000/movielens/raw/ratings/ratings.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "print(f\"üîç Raw films  : {movies_raw.count()}\")\n",
    "print(f\"üîç Raw notes  : {ratings_raw.count()}\")\n",
    "\n",
    "movies_raw.show(5, truncate=False)\n",
    "ratings_raw.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd9cd23-3a61-4499-81d8-b2ee5cd5a723",
   "metadata": {},
   "source": [
    "## 2. Analyse exploratoire rapide\n",
    "\n",
    "On regarde la distribution des notes pour d√©tecter d‚Äô√©ventuels outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8de6ede-d5f8-47e9-9583-8e4707aef714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean=3.526, StdDev=1.052\n",
      "+------+-------+\n",
      "|rating|  count|\n",
      "+------+-------+\n",
      "|   0.5| 239125|\n",
      "|   1.0| 680732|\n",
      "|   1.5| 279252|\n",
      "|   2.0|1430997|\n",
      "|   2.5| 883398|\n",
      "|   3.0|4291193|\n",
      "|   3.5|2200156|\n",
      "|   4.0|5561926|\n",
      "|   4.5|1534824|\n",
      "|   5.0|2898660|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stats = ratings_raw.select(\n",
    "    mean(\"rating\").alias(\"mean\"),\n",
    "    stddev(\"rating\").alias(\"stddev\")\n",
    ").first()\n",
    "mean_rating, stddev_rating = stats[\"mean\"], stats[\"stddev\"]\n",
    "print(f\"Mean={mean_rating:.3f}, StdDev={stddev_rating:.3f}\")\n",
    "\n",
    "ratings_raw.groupBy(\"rating\").count().orderBy(\"rating\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347fcc79-ba65-4258-8121-6b9d8a344842",
   "metadata": {},
   "source": [
    "## 3. Strat√©gie de nettoyage avanc√©\r\n",
    "\r\n",
    "Pour maximiser la qualit√© de l‚ÄôALS, on :\r\n",
    "1. Calcule le **z-score** des notes par utilisateur et exclut tout `|z| > 3`.  \r\n",
    "2. Filtre ensuite les notes hors de l‚Äôintervalle global `[Œº ¬± 3œÉ]`.  \r\n",
    "3. √âlimine les utilisateurs ayant < 20 interactions nettes et les films ayant < 50 interactions nettes.  \r\n",
    "4. Supprime doublons et nulls.\r\n",
    "\r\n",
    "Cette double d√©tection d‚Äôoutliers (global + par utilisateur) rendra le mod√®le plus robuste aux comportements extr√™mes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd5e50-6ec0-448a-89a8-22228eabb377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes avant nettoyage : 20000263\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, stddev, expr\n",
    "\n",
    "# 1. Stats globales\n",
    "stats = ratings_raw.select(mean(\"rating\").alias(\"Œº\"), stddev(\"rating\").alias(\"œÉ\")).first()\n",
    "Œº, œÉ = stats[\"Œº\"], stats[\"œÉ\"]\n",
    "\n",
    "# 2. Z-score par utilisateur\n",
    "user_stats = ratings_raw.groupBy(\"userId\") \\\n",
    "    .agg(avg(\"rating\").alias(\"Œº_u\"), stddev(\"rating\").alias(\"œÉ_u\"))\n",
    "\n",
    "ratings_z = ratings_raw.join(user_stats, \"userId\") \\\n",
    "    .withColumn(\"z_score\", (col(\"rating\") - col(\"Œº_u\"))/col(\"œÉ_u\"))\n",
    "\n",
    "# 3. Filtrage z-score et global\n",
    "clean1 = ratings_z.filter((col(\"z_score\").between(-3,3))) \\\n",
    "    .filter((col(\"rating\") >= Œº - 3*œÉ) & (col(\"rating\") <= Œº + 3*œÉ))\n",
    "\n",
    "# 4. Compter interactions nettes\n",
    "user_counts = clean1.groupBy(\"userId\").count().alias(\"user_count\")\n",
    "movie_counts = clean1.groupBy(\"movieId\").count().alias(\"movie_count\")\n",
    "\n",
    "# 5. Exclure les petits volumes\n",
    "clean2 = clean1.join(user_counts.filter(col(\"count\")>=20), \"userId\") \\\n",
    "               .join(movie_counts.filter(col(\"count\")>=50), \"movieId\")\n",
    "\n",
    "# 6. Suppression nulls/doublons\n",
    "ratings_clean = clean2.dropna(how=\"any\", subset=[\"userId\",\"movieId\",\"rating\",\"timestamp\"]) \\\n",
    "                      .dropDuplicates([\"userId\",\"movieId\",\"timestamp\"])\n",
    "\n",
    "print(f\"Notes avant nettoyage : {ratings_raw.count()}\")\n",
    "print(f\"Notes apr√®s nettoyage : {ratings_clean.count()}\")\n",
    "ratings_clean.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9e980a-9ff2-4629-ae2a-cda8658f820e",
   "metadata": {},
   "source": [
    "## 4. Enrichissement temporel\n",
    "\n",
    "Extraire ann√©e, mois, jour et convertir `timestamp` en date si n√©cessaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda0fa2-67ae-405f-8154-9b082831fbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_enriched = (ratings_clean\n",
    "    .withColumn(\"year\",  year(col(\"timestamp\")))\n",
    "    .withColumn(\"month\", month(col(\"timestamp\")))\n",
    "    .withColumn(\"day\",   dayofmonth(col(\"timestamp\")))\n",
    ")\n",
    "ratings_enriched.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af28a42f-bac7-45c8-87e4-3af78b55711a",
   "metadata": {},
   "source": [
    "## 5. R√©duction du nombre de partitions Parquet\r\n",
    "\r\n",
    "Pour √©viter de g√©n√©rer des milliers de petits fichiers, on repartitionne avant √©criture :\r\n",
    "\r\n",
    "- **Movies** : 1 partition  \r\n",
    "- **Ratings** : `year,month,day`, mais on limite √† ~ 10 partitions en coales√ßant sur la date.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c230f4da-ef73-45fe-8866-a2a27c290ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Movies en 1 unique Parquet\n",
    "movies_raw.repartition(1) \\\n",
    "    .write.mode(\"overwrite\") \\\n",
    "    .parquet(\"hdfs://namenode:9000/movielens/processed/batch/movies\")\n",
    "\n",
    "# Ratings coalesc√©es √† 10 fichiers max\n",
    "ratings_enriched \\\n",
    "    .coalesce(10) \\\n",
    "    .write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\",\"month\",\"day\") \\\n",
    "    .parquet(\"hdfs://namenode:9000/movielens/processed/batch/ratings\")\n",
    "\n",
    "print(\"Partitionnement et √©criture termin√©s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc73feb-d316-4e37-8fa7-9342ecb8ec75",
   "metadata": {},
   "source": [
    "## 6. Validation des Parquet\n",
    "\n",
    "V√©rification rapide des `count()` et aper√ßu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbcd943-cc1a-414e-b262-6730c60ea143",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = spark.read.parquet(\"hdfs://namenode:9000/movielens/processed/batch/movies\")\n",
    "df_ratings = spark.read.parquet(\"hdfs://namenode:9000/movielens/processed/batch/ratings\")\n",
    "\n",
    "print(f\"‚úîÔ∏è Films Parquet  : {df_movies.count()}\")\n",
    "print(f\"‚úîÔ∏è Notes Parquet  : {df_ratings.count()}\")\n",
    "\n",
    "df_movies.show(5, truncate=False)\n",
    "df_ratings.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73996784-b58c-423a-a953-bbd301ef91ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
