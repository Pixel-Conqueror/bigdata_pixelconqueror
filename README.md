# üé¨ Syst√®me de Recommandation de Films

## üåç Vue d'ensemble

Ce projet impl√©mente un syst√®me de recommandation de films utilisant une architecture Big Data avec :

- **Hadoop HDFS** pour le stockage distribu√©
- **Spark** pour le traitement des donn√©es
- **Kafka** pour le streaming en temps r√©el
- **MongoDB** pour le stockage des donn√©es nettoy√©es
- **ALS (Alternating Least Squares)** pour l'algorithme de recommandation

## üèóÔ∏è Architecture

![Architecture du syst√®me](docs/full%20archi.svg)

Le syst√®me est compos√© de plusieurs composants :

- **Frontend** (React) : Interface utilisateur pour visualiser les recommandations
- **Backend** (Python) : API REST pour g√©rer les requ√™tes
- **MongoDB** : Base de donn√©es pour stocker les donn√©es nettoy√©es
- **Hadoop HDFS** : Stockage distribu√© des donn√©es brutes
- **Spark** : Traitement des donn√©es et entra√Ænement du mod√®le
- **Kafka** : Gestion des flux de donn√©es en temps r√©el
- **Jupyter Notebook** : Environnement de d√©veloppement et d'exp√©rimentation

## üöÄ D√©marrage rapide

### Pr√©requis

- Docker et Docker Compose
- Python 3.x

### Installation

Vous avez le choix entre deux approches :

#### 1. Approche automatis√©e (recommand√©e)

```bash
# Pipeline compl√®te
make pipeline

# ou version all√©g√©e
make pipeline_light
```

#### 2. Approche manuelle (√©tape par √©tape)

1. **Lancer l'environnement Docker**

```bash
make up
```

2. **Initialiser HDFS et ing√©rer les donn√©es**

```bash
make init_hdfs
```

3. **Nettoyer et enrichir les donn√©es (ETL)**

```bash
make batch_etl
# ou version all√©g√©e
make batch_etl_light
```

4. **Charger les donn√©es dans MongoDB**

```bash
make load_to_mongo
```

5. **Entra√Æner le mod√®le ALS**

```bash
make train_als
```

6. **G√©n√©rer les recommandations**

```bash
make generate_recs
```

7. **Lancer le streaming**

```bash
make streaming
```

## üõ†Ô∏è Commandes utiles

### Gestion de l'environnement

```bash
# Arr√™ter l'environnement
make down

# Voir les logs
make logs
```

### Backend

```bash
# Acc√©der au shell du backend
make backend-shell

# Voir les logs du backend
make backend-logs
```

### MongoDB

```bash
# Acc√©der au shell MongoDB
make mongo-shell
```

### Kafka

```bash
# Lister les topics Kafka
make kafka-topics
```

### Jupyter

```bash
# Obtenir le token Jupyter
make jupyter-token
```

## üìä Architecture

- **Frontend** : http://localhost:3000
- **Backend API** : http://localhost:5001
- **MongoDB** : localhost:27017
- **Hadoop HDFS Namenode UI** : http://localhost:9870
- **Spark Master UI** : http://localhost:8080
- **Kafka** : localhost:9092
- **Jupyter Notebook** : http://localhost:8888

## üìÅ Structure du projet

```
/
|-- Dockerfile
|-- docker-compose.yml
|-- Makefile
|-- requirements.txt
|-- config/
|   |-- hadoop/
|-- scripts/
    |-- etl_batch.py
    |-- etl_batch_light.py
    |-- generate_all_recommendations.py
    |-- ingest_to_mongo.py
    |-- init_hdfs.sh
    |-- streaming_recommendations.py
    |-- test_update_user.py
    |-- train_als.py
```

## üìÑ Scripts et leurs fonctionnalit√©s

- **etl_batch.py** : Script de nettoyage des donn√©es en mode batch
- **etl_batch_light.py** : Version all√©g√©e du nettoyage des donn√©es
- **generate_all_recommendations.py** : G√©n√©ration des recommandations pour tous les utilisateurs avec leurs notes de base
- **ingest_to_mongo.py** : Insertion des donn√©es nettoy√©es en base MongoDB
- **init_hdfs.sh** : Initialisation d'Hadoop et d√©pot des fichiers dans HDFS
- **streaming_recommendations.py** : Consumer Kafka qui attend des JSON pour mettre √† jour les recommandations
- **test_update_user.py** : Producer qui g√©n√®re des notes pour un utilisateur donn√©
- **train_als.py** : Entra√Ænement du mod√®le de recommandation ALS

## üîî Notes importantes

- Assurez-vous d'avoir suffisamment de m√©moire disponible pour Spark (4GB minimum)
- Les donn√©es doivent √™tre correctement format√©es avant l'ingestion
- Le mod√®le ALS n√©cessite des donn√©es nettoy√©es pour un bon entra√Ænement

---

Made with ‚ù§Ô∏è by Robin, Thomas et Sonny
